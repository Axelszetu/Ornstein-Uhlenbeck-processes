---
title: "Software documentation for Sparse inference for Ornstein Uhlenbeck processes"
author: "Axel Ming Szetu"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
We will use the follwowing packages:
```{r}
library(tidyverse)
library(ggplot2)
library(reshape2)
library(lpSolve)
library(Matrix)
library(extraDistr) #for bernoulli distribution
library(MASS)
library(expm)
theme_set(theme_minimal())
```

# Methods

## d-dimansional Ornstein-Uhlenbeck processes

### Generation of sparse drift matrix
The function for generating d-dimensional drift matrices with eigenvalues with strictly positive real parts:
```{r}
make_drift_matrix <- function(d, sparsity){
  A <- diag(d)
  n <- floor((d^2)*sparsity) - d
  values <- rbern(n = n, prob = 0.5)
  values <- (values - 0.5)*2
  
  #browser()
  values_inserted <- 0
  while(values_inserted < n){
    coord_found <- 0
    attempts <- 0
    while(coord_found == 0){
      coords <- ceiling(runif(n = 2, min = 0, max = d))
      if (A[coords[1],coords[2]] == 0){
        A[coords[1],coords[2]] <- values[values_inserted+1]
        coord_found <- 1
      }
    }
    
    if (sum(Re(eigen(A)$values) > 0) == d){
      values_inserted <- values_inserted + 1
    }
    else{
      A[coords[1],coords[2]] <- 0
    }
    attemps <- attempts + 1
    if(attempts %% 100 == 0){
      #cat(sum(abs(A)), "of", d^2, "found", "\n")
    }
  }
  A
}
```

### Numerical simulation
The function for simulating a d-dimensional Ornstein-Uhlenbeck process with specified drift parameter, time scale and number of intermediate points. Initialization is currently done by letting the sumulation run for some time before starting observation.

```{r}
SimOU <- function(A0, t = 10, N = 1000, burn = 1000){
  d <- ncol(A0)
  #We need a d-dmiensional brownian motion.
  samples = rnorm(n = d*(N+burn), mean = 0, sd = sqrt(t/N))
  increment_matrix <- matrix(data = samples, nrow = d, ncol = N+burn, byrow = T)
  #Create matrix to contain OU process
  OU <- matrix(nrow = d, ncol = N+1+burn)
  
  #Initializing
  #Computation of covariance of stationary distribution
  
  #init <- C_infty(A_0)
  #OU[,1] <- init
  
  OU[,1] <- numeric(d)
  
  #Computation of reusable constant
  reversion_factor <- diag(d)-A0*(t/N)
  #Computation of OU paths
  for (i in 2:(N+1+burn)){
    OU[,i] <- reversion_factor%*%OU[,i-1] + increment_matrix[,i-1]
  }
  OU <- OU[,burn:(N+burn)]
}
```

### Sample paths
The following generates a drift matrix A_0 of dimension 8x8 and simulates a path from an OU process with drift matrix A_0.

```{r paths example}
d <- 8
sparsity <- 0.3
t <- 300
N <- 50000
burn <- 80000
set.seed(1)
A_0 <- make_drift_matrix(d = d, sparsity = sparsity)
sample_path <- SimOU(A_0, t = t, N = N, burn = burn)

plot_data <- as.data.frame(t(sample_path))
plot_data$t <- seq(from = 0, to = t, by = t/N)
melded_plot_data <- melt(plot_data, id.vars = "t")
ggplot(melded_plot_data, aes( x=t, y=value, colour=variable, group=variable )) + 
  geom_line() + theme_minimal()
```

## Maximum likelihood estimation for vector Ornstein-Uhlenbeck processes
Here we generate the drift matrix and OU_process that we will be working with for the remaining project.
```{r make A and process}
d <- 20
t <- 300
N <- 500000
burn <- 100000
dt <- t/N
sparsity <- 0.3

set.seed(1)
A <- make_drift_matrix(d = d, sparsity = sparsity)

process <- SimOU(A0 = A, t = t, N = N, burn  = burn)
```

The following defines the function to to compute the maximum likelihood estimator analytically.
```{r MLE definition}
OU_MLE_analytical <- function(X, dt){
  d <- nrow(X)
  N <- ncol(X)
  
  #Create matrix of increments
  dX <- apply(X, MARGIN = 1, FUN = diff)
  #Transpose it, because apply returns the result of each application as a column.
  dX <- t(dX)
  
  #Create matrix to iteratively contain terms of integral sum
  integral_matrix_1 <- matrix(data = 0, nrow = d, ncol = d)
  
  #Compute each step of integral sum
  for (i in 1:(N-1)){
    M <- dX[,i]%*%t(X[,i])
    integral_matrix_1 <- integral_matrix_1 + M
  }
  
  integral_matrix_2 <- matrix(data = 0, nrow = d, ncol = d)
  for (i in 1:(N-1)){
    M <- X[,i]%*%t(X[,i])*dt
    integral_matrix_2 <- integral_matrix_2 + M
  }
  integral_matrix_2_inverted <- solve(integral_matrix_2)
  
  A_hat <- -1 * integral_matrix_1%*%integral_matrix_2_inverted
  A_hat
}
```

We now compute the MLE of the simulated process and compare it to the true drift matrix with a heatmap of the covariance matrix.

```{r MLE computation and heatmap}
A_hat <- OU_MLE_analytical(X = process, dt = dt)
```
```{r}
limit <- c(-1.3, 1.3)
A_melt <- melt(t(A), varnames = c("Column", "Row"))
ggplot(data = A_melt, mapping = aes(x = Column, y = Row, fill = value)) + geom_tile(color = "white") +
  scale_fill_gradient2(low = rgb(0,0,219,maxColorValue = 255), high = rgb(219,0,0,maxColorValue = 255), mid = "white",
                       midpoint = 0, limit = limit, space = "Lab", 
                       name="Drift") +
  scale_y_reverse() + coord_fixed() + ggtitle("A_0")

A_hat_melt <- melt(t(A_hat), varnames = c("Column", "Row"))
ggplot(data = A_hat_melt, mapping = aes(x = Column, y = Row, fill = value)) + geom_tile(color = "white") +
  scale_fill_gradient2(low = rgb(0,0,219,maxColorValue = 255), high = rgb(219,0,0,maxColorValue = 255), mid = "white",
                       midpoint = 0, limit = limit, space = "Lab", 
                       name="Drift") +
  scale_y_reverse() + coord_fixed() + ggtitle("A_MLE")
```

## The Lasso estimator

### Numerical evaluation of likelihood function, computation of Lasso estimator and performance of Lasso estimator.

We find the solution to the Lasso minimization problem by passing a function that evaluate Lasso score to an optimization routine.
The following chunk defines functions for evaluating likelihood and optimizing lasso score.

```{r}
lasso_score_trace <- function(A, B, C, lambda, penalization = "L1"){
  d <- nrow(B)
  A <- matrix(data = A, nrow = d, ncol = d)
  
  if(penalization == "L1"){
    one_norm <- sum(abs(A))/(d^2)
    penalty <- lambda*one_norm
  }
  if(penalization == "F"){
    F_norm <- sum(A^2)/d^2
    penalty <- lambda*F_norm
  }
  
  likelihood <- sum(diag(B%*%t(A))) + sum(diag(A%*%C%*%t(A))) + penalty
  likelihood
}

OU_Lasso <- function(X, dt, lambda, penalization = "L1"){
  d <- nrow(X)
  N <- ncol(X)
  dX <- apply(X, MARGIN = 1, FUN = diff)
  dX <- t(dX)
  t <- N*dt
  
  B <- matrix(data = 0, nrow = d, ncol = d)
  for (i in (1:(N-1))){
    M <- dX[,i]%*%t(X[,i])
    B <- B + M
  }
  B_1 <- B/(N-1)
  
  C <- matrix(data = 0, nrow = d, ncol = d)
  for (i in (1:N)){
    M <- X[,i]%*%t(X[,i])
    C <- C + M
  }
  C <- C*dt
  C_1 <- C/(2*N)
  
  objective <- function(A){
    lasso_score_trace(A, B_1, C_1, lambda = lambda, penalization = "L1")
  }
  
  gradient <- function(A){
    A <- matrix(data = A, nrow = d, ncol = d)
    M_1 <- B
    M_2 <- C
    as.vector((A%*%M_2 - M_1)/t)
  }
  
  par <- diag(d)
  par <- as.vector(par)
  optimal_pars_vector <- lbfgs(call_eval = objective, call_grad = gradient, vars = par, orthantwise_c = lambda)
  optimal_pars_matrix <- matrix(data = optimal_pars_vector, nrow = d, ncol = d)
  optimal_pars_matrix
}
```

We now compute the Lasso estimator $A_L$.
```{r}
lambda <- 0.005
A_L1 <- OU_Lasso(X = process, dt = dt, lambda = lambda)
A_L2 <- OU_Lasso(X = process, dt = dt, lambda = lambda, penalization = "F")
```

Below we make make the heatmap plots of A_L1 and A_L2.
```{r}
A_L1_melt <- melt(t(A_L1), varnames = c("Column", "Row"))
ggplot(data = A_L1_melt, mapping = aes(x = Column, y = Row, fill = value)) + geom_tile(color = "white") +
  scale_fill_gradient2(low = rgb(0,0,219,maxColorValue = 255), high = rgb(219,0,0,maxColorValue = 255), mid = "white",
                       midpoint = 0, limit = limit, space = "Lab", 
                       name="Drift") +
  scale_y_reverse() + coord_fixed() + ggtitle("A_L1")

A_L2_melt <- melt(t(A_L2), varnames = c("Column", "Row"))
ggplot(data = A_L2_melt, mapping = aes(x = Column, y = Row, fill = value)) + geom_tile(color = "white") +
  scale_fill_gradient2(low = rgb(0,0,219,maxColorValue = 255), high = rgb(219,0,0,maxColorValue = 255), mid = "white",
                       midpoint = 0, limit = limit, space = "Lab", 
                       name="Drift") +
  scale_y_reverse() + coord_fixed() + ggtitle("A_L2")
```
### Cross-validation
The following is a function that selects the optimal lambda from a provided list by cross validation.
It allows for any estimator to be used - as long as it returns a drift matrix of correct dimensions.
```{r}
likelihood_evaluator_trace <- function(A, B, C){
  d <- nrow(B)
  A <- matrix(data = A, nrow = d, ncol = d)
  
  likelihood <- sum(diag(B%*%t(A))) + sum(diag(A%*%C%*%t(A)))
  likelihood
}

cross_validator <- function(X, dt, f, pars, split = 0.8){
  #Setting constant values
  no_pars <- length(pars)
  scores <- numeric(length = no_pars)
  #estimator <- f
  d <- nrow(X)
  N <- ncol(X)
  
  #Splitting the sample
  split_value <- ceiling(N*split)
  training_set <- X[,(1:split_value)]
  validation_set <- X[,((split_value+1):N)]
  
  #The following will be used in the evaluation likelihood of validation set
  dtraining_set <- apply(training_set, MARGIN = 1, FUN = diff)
  dtraining_set <- t(dtraining_set)
  
  B <- matrix(data = 0, nrow = d, ncol = d)
  for (i in (1:(split_value-1))){
    M <- dtraining_set[,i]%*%t(training_set[,i])
    B <- B + M
  }
  B <- B/(split_value-1)
  
  C <- matrix(data = 0, nrow = d, ncol = d)
  for (i in (1:split_value)){
    M <- training_set[,i]%*%t(training_set[,i])
    C <- C + M
  }
  C <- C/(2*split_value)
  C <- C*dt
  
  #For each lambda, estimate A_lambda and compute (negative log-)likelihood or validation set under A_lambda
  for (i in (1:no_pars)){
    A_lambda <- f(training_set, dt, pars[i])
    scores[i] <- likelihood_evaluator_trace(A_lambda, B, C)
  }
  
  #Extracting optimal pars value
  index_of_minimum <- which.min(scores)
  optimal_parameter <- pars[index_of_minimum]
  optimal_parameter
}
```
We perform cross validation on the lambda values exp(4), exp(3), ... exp(-7)

```{r}
bases <- (4:-7)
lambdas <- exp(bases)
lambda_L <- cross_validator(process, dt = dt, f = OU_Lasso, pars = lambdas, split = 0.8)
```
We compute the Lasso estimator with the found value of lambda
```{r}
A_L_CV <- OU_Lasso(X = process, dt = dt, lambda = exp(-7), penalization = "L1")
A_L_CV_melt <- melt(t(A_L_CV), varnames = c("Column", "Row"))
ggplot(data = A_L_CV_melt, mapping = aes(x = Column, y = Row, fill = value)) + geom_tile(color = "white") +
  scale_fill_gradient2(low = rgb(0,0,219,maxColorValue = 255), high = rgb(219,0,0,maxColorValue = 255), mid = "white",
                       midpoint = 0, limit = limit, space = "Lab", 
                       name="Drift") +
  scale_y_reverse() + coord_fixed() + ggtitle("A_L_CV")
```
## The Dantzig estimator
We use the lpSolve package for the simplex algorithm.
The following function computes the Dantzig estimator given a data and a regularization parameter lambda.
```{r}
OU_Dantzig <- function(X, dt, lambda){
  d <- nrow(X)
  N <- ncol(X)
  t <- dt*N
  dX <- apply(X, MARGIN = 1, FUN = diff)
  dX <- t(dX)
  
  M_1 <- matrix(data = 0, nrow = d, ncol = d)
  for (i in 1:(N-1)){
    M <- dX[,i]%*%t(X[,i])
    M_1 <- M_1 + M
  }
  
  M_2 <- matrix(data = 0, nrow = d, ncol = d)
  for (i in 1:(N-1)){
    M <- X[,i]%*%t(X[,i])*dt
    M_2 <- M_2 + M
  }
  
  A_mle <- -1 * M_1%*%solve(M_2)
  
  c <- rep(1, 2*d^2)
  
  diag_block <- bdiag(replicate(d, t(M_2), simplify = FALSE))
  B_left <- rbind(diag_block, -diag_block)
  B_right <- rbind(-diag_block, diag_block)
  B <- cbind(B_left, B_right)
  B <- as.matrix(B)
  
  dir <- rep("<=", 2*d^2)
  
  M_ij_vector <- as.vector(t(M_1))
  rhs_top <- -M_ij_vector + t*lambda
  rhs_bot <- M_ij_vector + t*lambda
  rhs <- c(rhs_top, rhs_bot)
  
  lp_problem <- lp(direction = "min", objective.in = c, const.mat = B, const.dir = dir, const.rhs = rhs)
  
  solution <- lp_problem$solution
  A_pos_values <- solution[1:d^2]
  A_pos <- matrix(data = A_pos_values, nrow = d, ncol = d, byrow = T)
  A_neg_values <- solution[(d^2 + 1):(2*d^2)]
  A_neg <- matrix(data = A_neg_values, nrow = d, ncol = d, byrow = T)
  
  A <- A_pos - A_neg
  
  A
}
```
We calibrate lambda by the same cross validation we used earlier and compute the dantzig estimator for the found lambda.
```{r}
lambdas_D <- exp((4:-8))
lambda_D <- cross_validator(X = process, dt = dt, f = OU_Dantzig, pars = lambdas_D, split = 0.8)
A_D <- OU_Dantzig(X = process, dt = dt, lambda = lambda_D)
```
```{r}
A_D_melt <- melt(t(A_D), varnames = c("Column", "Row"))
ggplot(data = A_D_melt, mapping = aes(x = Column, y = Row, fill = value)) + geom_tile(color = "white") +
  scale_fill_gradient2(low = rgb(0,0,219,maxColorValue = 255), high = rgb(219,0,0,maxColorValue = 255), mid = "white",
                       midpoint = 0, limit = limit, space = "Lab", 
                       name="Drift") +
  scale_y_reverse() + coord_fixed() + ggtitle("A_D")
```






# Results